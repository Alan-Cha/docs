[{"uri":"https://iter8.tools/docs/archive/v1.0.0/getting-started/canary/","title":"Getting started with canary testing","tags":[],"description":"","content":"This tutorial shows how iter8 can be used to perform a canary release by gradually shifting traffic from one version of a microservice to another while evaluating the behavior of the new version. Traffic is fully shifted only if the behavior the candidate version meets specified acceptance criteria.\nThis tutorial has seven steps, which are meant to be tried in order. You will learn:\n how to perform a canary rollout with iter8; and how to define different success criteria for iter8 to analyze canary releases and determine success or failure;  The tutorial is based on the Bookinfo sample application distributed with Istio. This application comprises 4 microservies: productpage, details, reviews, and ratings. Of these, productpage is a user-facing service while the others are backend services.\nThis tutorial assumes you have already installed iter8 (including Istio). If not, do so using the instructions here.\nDeploy the Bookinfo application To deploy the Bookinfo application, create a namespace configured to enable auto-injection of the Istio sidecar. You can use whatever namespace name you wish. By default, the namespace bookinfo-iter8 is created.\nexport NAMESPACE=bookinfo-iter8 curl -s https://raw.githubusercontent.com/iter8-tools/docs/master/static/tutorials/namespace.yaml \\  | sed \u0026#34;s#bookinfo-iter8#$NAMESPACE#\u0026#34; \\  | kubectl apply -f - Next, deploy the application:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/docs/master/static/tutorials/bookinfo-tutorial.yaml You should see pods for each of the four microservices:\nkubectl --namespace $NAMESPACE get pods Note that we deployed version v2 of the reviews microsevice; that is, reviews-v2. Each pod should have two containers, since the Istio sidecar was injected into each.\nExpose the Bookinfo application Expose the Bookinfo application by defining an Istio Gateway and VirtualService:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/docs/master/static/tutorials/bookinfo-gateway.yaml You can inspect the created resources:\nkubectl --namespace $NAMESPACE get gateway,virtualservice Note that the service has been associated with a fake host, bookinfo.example.com for demonstration purposes.\nVerify access to Bookinfo To access the application, determine the ingress IP and port for the application. You can do so by following steps 3 and 4 of the Istio instructions here to set the environment variables GATEWAY_URL. You can then check if you can access the application with the following curl command:\ncurl --header \u0026#39;Host: bookinfo.example.com\u0026#39; -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; If everything is working, the command above should return 200. Note that the curl command above sets the Host header to match the host we associated the VirtualService with (bookinfo.example.com).\nNote: If you want to access the application from your browser, you will need to set this header using a browser plugin.\nGenerate load To simulate user requests, use a command such as the following:\nwatch -n 0.1 \u0026#39;curl --header \u0026#34;Host: bookinfo.example.com\u0026#34; -s \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; | grep -i \u0026#34;color=\\\u0026#34;\u0026#34;\u0026#39; This command requests the productpage microservice 10 times per second. In turn, this causes about the same frequency of requests against the backend microservice. We filter the response to see the color being used to display the \u0026ldquo;star\u0026rdquo; rating of the application. The color varies between versions giving us a visual way to distinguish between them.\nCreate a canary Experiment We will now define a canary experiment to rollout version v3 of the reviews application. These versions are identical except for the color of the stars that appear on the page. In version v3 they are red. This can be seen in the inspected in the output of the above watch command. As version v3 is rolled out, you should see the color change.\nTo describe a canary rollout, create an iter8 Experiment that identifies the original, or baseline version and the new, or candidate version and some evaluation criteria. For example:\napiVersion: iter8.tools/v1alpha2 kind: Experiment metadata: name: reviews-v3-rollout spec: service: name: reviews baseline: reviews-v2 candidates: [ \u0026#34;reviews-v3\u0026#34; ] criteria: - metric: iter8_mean_latency threshold: type: absolute value: 200 duration: maxIterations: 8 interval: 15s trafficControl: maxIncrement: 20 In this example, the target of the experiment is the service reviews. The baseline and candidate versions are specified using their Deployment names, reviews_v2 and reviews_v3, respectively. A single evaluation criteria is specified. It requires that the measurements of the metric iter8_mean_latency should all return values less than 200 milliseconds. The additional parameters control how long the experiment should run and how much traffic can be shifted to the new version in each interval. Details regarding these parameters are here.\nThe experiment can be created using the command:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/docs/master/static/tutorials/canary_reviews-v2_to_reviews-v3.yaml Inspection of the new experiment shows that it is paused because the specified candidate version cannot be found in the cluster:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Pause TargetsError: Missing Candidate Once the candidate version is deployed, the experiment will start automatically.\nDeploy the candidate version of the reviews service To deploy version v3 of the reviews microservice, execute:\nkubectl --namespace $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/docs/master/static/tutorials/reviews-v3.yaml Once its corresponding pods have started, the Experiment will show that it is progressing:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Progressing false reviews-v3 IterationUpdate: Iteration 0/8 completed At approximately 15 second intervals, you should see the interation number change. Traffic will gradually be shifted (in 20% increments) from version v2 to version v3. iter8 will quickly identify that the best version is the candidate, reviews-v3 and that it is confident that this choice will be the final choice (by indicating that a winner has been found:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Progressing true reviews-v2 IterationUpdate: Iteration 3/8 completed When the experiment is finished (about 2 minutes), you will see that all traffic has been shifted to the winner, reviews-v3:\nkubectl --namespace $NAMESPACE get experiment NAME TYPE HOSTS PHASE WINNER FOUND CURRENT BEST STATUS reviews-v3-rollout Canary [reviews] Completed true reviews-v3 ExperimentCompleted: Traffic To Winner Cleanup To clean up, delete the namespace:\nkubectl delete namespace $NAMESPACE Other things to try (before cleanup) Inspect progress using Grafana Coming soon\nInspect progress using Kiali Coming soon\nAlter the duration of the experiment The progress of an experiment can be impacted by duration and trafficControl parameters:\n duration.interval defines how long each test interval should be (default: 30 seconds) duration.maxIterations identifies what the maximum number of iterations there should be (default: 100) trafficControl.maxIncrement identifies the largest change (increment) that will be made in the percentage of traffic sent to a candidate (default: 2 percent)  The impact of the first two parameters on the duration of the experiment are clear. Restricting the size of traffic shifts limits how quickly an experiment can come to a decision about a candidate.\nTry a version that fails the criteria Version v4 of the reviews service is a modification that returns after a 5 second delay. If you try this version as a candidate, you should see the canary experiment reject it and choose the baseline version as the winner.\nFor your reference:\n A YAML for the deployment reviews-v4 is: https://raw.githubusercontent.com/iter8-tools/docs/master/static/tutorials/reviews-v4.yaml A YAML for an canary experiment from reviews-v3 to reviews-v4 is: https://raw.githubusercontent.com/iter8-tools/docs/master/static/tutorials/canary_reviews-v3_to_reviews-v4.yaml  Try a version which returns errors Coming soon\nTry with a user-facing service Coming soon\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/","title":"Homepage","tags":[],"description":"","content":"Deliver better software in the cloud Use iter8\u0026rsquo;s analytics-driven continuous experimentation for reliable and frequent releases of high-quality microservices on Kubernetes.\n Automate canary releases Use advanced statistical algorithms to assess key metrics for your service and progressively shift traffic to the winning release.\n Launch experiments rapidly With iter8\u0026rsquo;s Kiali UI, you can create and launch canary release experiments for your service in seconds, and observe and control these experiments in real-time.\n Analyze long-term trends Analyze how key metrics for your service have evolved over multiple releases using iter8-trend and Grafana.\n Explore iter8\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/about/","title":"Iter8 enables statistically robust continuous experimentation of microservices in your CI/CD pipelines","tags":[],"description":"","content":"Use an iter8 experiment to safely expose competing versions of a service to application traffic, gather in-depth insights about key performance and business metrics for your microservice versions, and intelligently rollout the best version of your service.\nIter8\u0026rsquo;s expressive model of cloud experimentation supports a variety of CI/CD scenarios. Using an iter8 experiment, you can:\n Run a performance test with a single version of a microservice. Perform a canary release with two versions, a baseline and a candidate. Iter8 will shift application traffic safely and gradually to the candidate, if it meets the criteria you specify in the experiment. Perform an A/B test with two versions \u0026ndash; a baseline and a candidate. Iter8 will identify and shift application traffic safely and gradually to the winner, where the winning version is defined by the criteria you specify in the experiment. Perform an A/B/N test with multiple versions \u0026ndash; a baseline and multiple candidates. Iter8 will identify and shift application traffic safely and gradually to the winner.  Under the hood, iter8 uses advanced Bayesian learning techniques coupled with multi-armed bandit approaches to compute a variety of statistical assessments for your microservice versions, and uses them to make robust traffic control and rollout decisions.\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/getting-started/installation/kubernetes/","title":"Iter8 on Kubernetes and Istio","tags":[],"description":"","content":"These instructions show you how to set up iter8 on Kubernetes with Istio.\nPrerequisites  Kubernetes v1.11 or newer. Istio v1.1.5 and newer. Your Istio installation must have at least the istio-pilot as well as telemetry and Prometheus enabled.  Install iter8 on Kubernetes iter8 has two components, iter8_analytics and iter8_controller. To install them, follow the instructions below. For additional considerations when installing iter8 on Red Hat OpenShift, check out these instructions.\nQuick installation To install iter8 with the default settings, you can run the following install script:\ncurl -L -s https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/install.sh \\ | /bin/bash - Customized installation via Helm charts In case you need to customize the installation of iter8, use the Helm charts listed below:\n  iter8-analytics: https://github.com/iter8-tools/iter8-analytics/releases/download/v0.2.1/iter8-analytics-helm-chart.tar\n  iter8-controller: https://github.com/iter8-tools/iter8-controller/releases/download/v0.2.1/iter8-controller-helm-chart.tar\n  Note on Prometheus: In order to make assessments, iter8-analytics needs to query metrics collected by Istio and stored on Prometheus. The default values for the helm chart parameters (used in the quick installation) point iter8-analytics to the Prometheus server at http://prometheus.istio-system:9090 (the default internal Kubernetes URL of Prometheus installed as an Istio addon) without specifying any need for authentication. If your Istio installation is shipping metrics to a different Prometheus service, or if you need to configure authentication to access Prometheus, you need to set appropriate iter8-analytics Helm chart parameters. Look in the section metricsBackend of the Helm chart\u0026rsquo;s values.yaml file for details.\nNote on Istio Telemetry: When deploying iter8-controller using helm, make sure to set the parameter istioTelemetry to conform with your environment. Possible values are v1 or v2. Use v1 if the Istio mixer is not disabled. You can determine whether or not the mixer is disabled using this command:\nkubectl -n $ISTIO_NAMESPACE get cm istio -o json | jq .data.mesh | grep -o \u0026#39;disableMixerHttpReports: [A-Za-z]\\+\u0026#39; | cut -d \u0026#39; \u0026#39; -f2 Verify the installation After installing iter8-analytics and iter8-controller, you should see the following pods and services in the newly created iter8 namespace:\n$ kubectl get pods -n iter8 NAME READY STATUS RESTARTS AGE iter8-controller-5f54bb4b88-drr8s 1/1 Running 0 4s iter8-analytics-5c5758ccf9-p575b 1/1 Running 0 61s $ kubectl get svc -n iter8 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iter8-controller ClusterIP 172.21.62.217 \u0026lt;none\u0026gt; 443/TCP 20s iter8-analytics ClusterIP 172.21.106.44 \u0026lt;none\u0026gt; 80/TCP 76s Import iter8\u0026rsquo;s Grafana dashboard To enable users to see Prometheus metrics that pertain to their canary releases or A/B tests, iter8 provides a Grafana dashboard template. To take advantage of Grafana, you will need to import this template. To do so, first make sure you can access Grafana. In a typical Istio installation, you can port-forward Grafana from Kubernetes to your localhost\u0026rsquo;s port 3000 with the command below:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 After running that command, you can access Grafana\u0026rsquo;s UI at http://localhost:3000.Iter8 dashboard can be imported by:\ncurl -L -s https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/hack/grafana_install_dashboard.sh \\ | /bin/bash - Uninstall iter8 If you want to uninstall all iter8 components from your Kubernetes cluster, first delete all instances of Experiment from all namespaces. Then, you can delete iter8 by running the following command:\nkubectl delete -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/iter8-controller.yaml Note that this command will delete the Experiment CRD and wipe out the iter8 namespace, but it will not remove the iter8 Grafana dashboard if created.\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/reference/metrics/","title":"Iter8&#39;s metrics","tags":[],"description":"","content":"Coming soon!\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/getting-started/deployments/","title":"Automated canary releases with iter8 on Kubernetes and Istio","tags":[],"description":"","content":"This tutorial shows you how iter8 can be used to perform canary releases by gradually shifting traffic to a canary version of a microservice.\nThis tutorial has 5 parts, which are supposed to be tried in order. Here you will learn:\n how to perform a canary rollout with iter8; how to set different success criteria for iter8 to analyze canary releases and determine success or failure; how to have iter8 immediately stop an experiment as soon as a criterion is not met; how to use your own custom metrics in success criteria for canary analyses; and how iter8 can be used for canary releases of both internal and user-facing services.  The tutorial is based on the Bookinfo sample application that is distributed with Istio. This application comprises 4 microservices, namely, productpage, details, reviews, and ratings, as illustrated here. Please, follow our instructions below to deploy the sample application as part of the tutorial.\nYAML files used in the tutorial All Kubernetes YAML files you will need in this tutorial are in the iter8-controller repository here.\nPart 1: Successful canary release: reviews-v2 to reviews-v3 1. Deploy the Bookinfo application At this point, we assume that you have already followed the instructions to install iter8 on your Kubernetes cluster. The next step is to deploy the sample application we will use for the tutorial.\nFirst, let us create a bookinfo-iter8 namespace configured to enable auto-injection of the Istio sidecar:\nkubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/namespace.yaml Next, let us deploy the Bookinfo application:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-tutorial.yaml You should see the following pods in the bookinfo-iter8 namespace. Make sure the pods\u0026rsquo; status is \u0026ldquo;Running.\u0026rdquo; Also, note that there should be 2 containers in each pod, since the Istio sidecar was injected.\n$ kubectl get pods -n bookinfo-iter8 NAME READY STATUS RESTARTS AGE details-v1-68c7c8666d-m78qx 2/2 Running 0 64s productpage-v1-7979869ff9-fln6g 2/2 Running 0 63s ratings-v1-8558d4458d-rwthl 2/2 Running 0 64s reviews-v2-df64b6df9-ffb42 2/2 Running 0 63s We have deployed \u0026ldquo;version 2\u0026rdquo; of the reviews microservice, and version 1 of all others.\nLet us now expose the edge productpage service by creating an Istio Gateway for it.\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-gateway.yaml You should now see the Istio Gateway and VirtualService for productpage, as below:\n$ kubectl get gateway -n bookinfo-iter8 NAME AGE bookinfo-gateway 22s $ kubectl get vs -n bookinfo-iter8 NAME GATEWAYS HOSTS AGE bookinfo [bookinfo-gateway] [bookinfo.sample.dev] 27s As you can see above, we have associated Bookinfo\u0026rsquo;s edge service with a fake host, namely, bookinfo.sample.dev.\n2. Access the Bookinfo application To access the application, you need to determine the ingress IP and port for the application in your environment. You can do so by following steps 3 and 4 of the Istio instructions here to set the environment variables INGRESS_HOST, INGRESS_PORT, and GATEWAY_URL, which will capture the correct IP address and port for your environment. Once you have done so, you can check if you can access the application with the following command:\ncurl -H \u0026#34;Host: bookinfo.sample.dev\u0026#34; -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; If everything is working, the command above should show 200. Note that the curl command above sets the host header to match the host we associated the VirtualService with (bookinfo.sample.dev). If you want to access the application from your browser, you will need to set this header using a browser\u0026rsquo;s plugin of your choice.\n3. Generate load to the application Let us now generate load to the application, emulating requests coming from users. To do so, we recommend you run the command below on a separate terminal:\nwatch -n 0.1 \u0026#39;curl -H \u0026#34;Host: bookinfo.sample.dev\u0026#34; -Is \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34;\u0026#39; This command will send 10 requests per second to the application. Note that the environment variable GATEWAY_URL must have been set as per step 2 above. Among other things, the command output should show an HTTP code of 200, as below:\nHTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 5719 server: istio-envoy (...) 4. Configure a canary rollout for the reviews service At this point, Bookinfo is using version 2 of the reviews service (reviews-v2). Let us now use iter8 to automate the canary rollout of version 3 of this service (reviews-v3).\nFirst, we need to tell iter8 that we are about to perform this canary rollout. To that end, we create an Experiment configuration specifying the rollout details. In this tutorial, let us use the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v3-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v2 candidate: reviews-v3 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 8 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 200 sampleSize: 5 The configuration above specifies the baseline and candidate versions in terms of Kubernetes deployment names. The rollout is configured to last for 8 iterations (maxIterations) of 30s (interval). At the end of each iteration, if the candidate version meets the specified success criteria, the traffic sent to it will increase by 20 percentage points (trafficStepSize) up to 80% (maxTrafficPercentage). At the end of the last iteration, if the success criteria are met, the candidate version will take over from the baseline.\nIn the example above, we specified only one success criterion. In particular, we stated that the mean latency exhibited by the candidate version should not exceed the threshold of 200 milliseconds. At the end of each iteration, iter8-controller calls iter8-analytics, which in turn analyzes the metrics of interest (in this case, only mean latency) against the corresponding criteria. The number of data points analyzed during an experiment is cumulative, that is, it carries over from iteration to iteration.\nThe next step of this tutorial is to actually create the configuration above. To that end, you can either copy and paste the yaml above to a file and then run kubectl apply -n bookinfo-iter8 -f on it, or you can run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v2_to_reviews-v3.yaml You can verify that the Experiment object has been created as shown below:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Pause TargetsNotFound: Missing Candidate reviews-v2 100 reviews-v3 0 As you can see, iter8 is reporting that 100% of the traffic is sent to the baseline version (reviews-v2) and that the candidate (reviews-v3) is missing. As soon as the controller sees the candidate version, it will start the rollout. Next, let us deploy the candidate version to trigger the canary rollout.\n5. Deploy the canary version and start the rollout As soon as we deploy reviews-v3, iter8-controller will start the rollout. To deploy reviews-v3, you can run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v3.yaml Now, if you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v3:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v2 80 reviews-v3 20 At about every 30s you should see the traffic shift towards reviews-v3 by 20 percentage points.\n6. Check the Grafana dashboard You can also check a Grafana dashboard specific to the Experiment object corresponding to the rollout you are running. The URL to the Grafana dashboard for the experiment is the value of the field grafanaURL under the object\u0026rsquo;s status. One way to get the Grafana URL that you can paste to your browser is through the following command:\nkubectl get experiment reviews-v3-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 By default, the base URL given by iter8 to Grafana is http://localhost:3000. In a typical Istio installation, you can port-forward your Grafana from Kubernetes to your localhost\u0026rsquo;s port 3000 with the following command:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 Below is a screenshot of a portion of the Grafana dashboard showing the request rate and the mean latency for reviews-v2 and reviews-v3, right after the controller ended the experiment.\nNote how the traffic shifted towards the canary during the experiment. You can also see that the canary\u0026rsquo;s mean latency was way below the configured threshold of 200 milliseconds.\nPart 2: High-latency canary release: reviews-v3 to reviews-v4 At this point, you must have completed the part 1 of the tutorial successfully. You can confirm it as follows:\n$ kubectl get experiment reviews-v3-rollout -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 The command above\u0026rsquo;s output shows that reviews-v3 took over from reviews-v2 as part of the canary rollout performed before.\n1. Canary rollout configuration Now, let us set up a canary rollout for reviews-v4, using the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v4-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v3 candidate: reviews-v4 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 200 sampleSize: 5 The configuration above is pretty much the same we used in part 1, except that now the baseline version is reviews-v3 and the candidate is reviews-v4.\nTo create the above Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v3_to_reviews-v4.yaml You can list all Experiment objects like so:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Pause TargetsNotFound: Missing Candidate reviews-v3 100 reviews-v4 0 The output above shows the new object you just created, for which the candidate deployment reviews-v4 is missing. Let us deploy reviews-v4 next so that the rollout can begin.\n2. Deploy reviews-v4 and start the rollout As you have already seen, as soon as we deploy the candidate version, iter8-controller will start the rollout. This time, however, the candidate version (reviews-v4) has a performance issue preventing it from satisfying the success criteria in the experiment object. As a result, iter8 will roll back to the baseline version.\nTo deploy reviews-v4, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v4.yaml Now, if you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v4.\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v3 80 reviews-v4 20 However, unlike the previous rollout, traffic will not shift towards the candidate reviews-v4 because it does not meet the success criteria due to a performance problem. At the end of the experiment, iter8 rolls back to the baseline (reviews-v3), as seen below:\n$ kubectl get experiment reviews-v4-rollout -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 3. Check the Grafana dashboard As before, you can check the Grafana dashboard corresponding to the canary release of reviews-v4. To get the URL to the dashboard specific to this canary release, run the following command:\nkubectl get experiment reviews-v4-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 The dashboard screenshot above shows that the canary version (reviews-v4) consistently exhibits a high latency of 5 seconds, way above the threshold of 200 milliseconds specified in our success criterion, and way above the baseline version\u0026rsquo;s latency.\nPart 3: Error-producing canary release: reviews-v3 to reviews-v5 At this point, you must have completed parts 1 and 2 of the tutorial successfully. You can confirm it as follows:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 The command above\u0026rsquo;s output shows that reviews-v3 took over from reviews-v2 as part of the canary rollout performed before on part 1, and that it continues to be the current version after iter8 had determined that reviews-v4 was unsatisfactory.\n1. Canary rollout configuration Now, let us set up a canary rollout for reviews-v5, using the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v5-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v3 candidate: reviews-v5 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 200 sampleSize: 5 - metricName: iter8_error_rate toleranceType: delta tolerance: 0.02 sampleSize: 10 stopOnFailure: true The configuration above differs from the previous ones as follows. We added a second success criterion on the error-rate metric so that the canary version (reviews-v5) not only must have a mean latency below 200 milliseconds, but it also needs to have an error rate that cannot exceed the baseline error rate by more than 2%. That comparative analysis on a metric is specified as a delta tolerance type. Furthermore, the second success criterion sets the flag stopOnFailure, which means iter8 will roll back to the baseline as soon as the error rate criterion is violated and the minimum number of 10 data points is collected (sampleSize = 10).\nTo create the above Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v3_to_reviews-v5.yaml 2. Deploy reviews-v5 and start the rollout As you already know, as soon as we deploy the candidate version, iter8-controller will start the rollout. This time, the candidate version (reviews-v5) has a bug that causes it to return HTTP errors to its callers. As a result, iter8 will roll back to the baseline version based on the success criterion on the error-rate metric defined above.\nTo deploy reviews-v5, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v5.yaml If you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v5.\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v3 80 reviews-v5 20 Because review-v5 has an issue causing it to return HTTP errors, as per the success criteria we have specified the traffic will not shift towards it. Furthermore, because the error-rate success criteria indicated the need to stop on failure, without waiting for the entire duration of the experiment, iter8 will rollback to reviews-v3 quickly. You should see the following after several seconds:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 3. Check the Grafana dashboard As before, you can check the Grafana dashboard corresponding to the canary release of reviews-v5. To get the URL to the dashboard specific to this canary release, run the following command:\nkubectl get experiment reviews-v5-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 The dashboard screenshots above show that traffic to the canary version (reviews-v5) is quickly interrupted. Also, while the reviews-v5 latency is way below the threshold of 200 milliseconds we defined in the latency success criterion, its error rate is 100%, i.e., it generates errors for every single request it processes. That does not meet the error-rate success criterion we defined, which specified that the canary\u0026rsquo;s error rate must be within 2% of that of the baseline (reviews-v3) version. According to the dashboard, reviews-v3 produced no errors at all.\nPart 4: Using a custom metric At this point, you should have completed parts 1, 2, and 3 of the tutorial successfully. You can confirm it as follows:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 The command above\u0026rsquo;s output shows that reviews-v3 took over from reviews-v2 as part of the canary rollout performed before in part 1, and that it continued to be the current version of the reviews service after iter8 had determined that reviews-v4 was unsatisfactory. Similarly, as we saw in the previous part 3, the experiment to rollout reviews-v5 was aborted because of failure to satisfy the success criteria defined by the user.\nIn this tutorial, we will define a custom metric (one not provided by iter8 out of the box) and use it in the success criteria for a canary release.\nBy default iter8 provides a few metrics which you can see if you type:\n$ kubectl get configmap iter8config-metrics -n iter8 -oyaml In principle, any metric that can be derived from the data you have in your Prometheus database that might be meaningful to you in assessing the health of a service version can be used by iter8. Next, we are going to make iter8 aware of a metric that we will call iter8_90_perc_latency, which measures the 90th percentile latency of a service. In order to make iter8 aware of a new metric we need to add it to the iter8config-metrics config map. For the purposes of this tutorial, we will do so by running the following command:\nkubectl apply -n iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/iter8_metrics_extended.yaml Or, if using a newer version of Istio (1.5 or greater) with telemetry v2:\nkubectl apply -n iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/iter8_metrics_extended_telemetry-v2.yaml Note:  For additional information about how to add a new metric to the existing configuration please see this documentation.\n To verify that the new metric has been added to the configmap, you can check it again:\n$ kubectl get configmap iter8config-metrics -n iter8 -oyaml We will now configure an experiment to use this new metric for a canary release.\n1. Canary rollout configuration Now, let us set up a canary rollout for reviews-v6, using the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v6-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v3 candidate: reviews-v6 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_90_perc_latency toleranceType: threshold tolerance: 200 sampleSize: 5 The configuration uses the newly extended metric iter8_90_perc_latency. The success criteria asserts that the canary version (reviews-v6) must have the 90th percentile latency below 200 milliseconds.\nTo create the above Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v3_to_reviews-v6.yaml As usual, iter8 is waiting for the candidate version to be deployed:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 reviews-v6-rollout Pause TargetsNotFound: Missing Candidate reviews-v3 100 reviews-v6 0 2. Deploy reviews-v6 and start the rollout As soon as we deploy the candidate version, iter8-controller will start the rollout. This time, the candidate version (reviews-v6) is similar to the earlier reviews-v3 which behaved normally. As a result, iter8 will roll forward to the candidate version based on the success criterion on the newly extended metric defined above.\nTo deploy reviews-v6, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v6.yaml If you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v6.\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 reviews-v6-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v3 80 reviews-v6 20 At about every 30s you should see the traffic shift towards reviews-v6 by 20 percentage points.\nAt the end of the experiment, you will see that all traffic has been shifted to the canary version (reviews-v6)\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 reviews-v6-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v3 0 reviews-v6 100 3. Check the Grafana dashboard As before, you can check the Grafana dashboard corresponding to the canary release of reviews-v6. To get the URL to the dashboard specific to this canary release, run the following command:\nkubectl get experiment reviews-v6-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 You can also extend the Grafana Dashboard with the new metric by adding a new panel to the dashboard that looks as follows:\nOther configurations such as title, legend, etc can be varied as per the user\u0026rsquo;s preference.\nPart 5: User-facing Canary release: productpage-v1 to productpage-v2 1. Traffic configuration Consider the case now you want to rollout a new version of productpage deployment productpage-v2 and expose the service outside of the cluster to users through the host productpage.deployment.com. You will need to setup an Istio Gateway:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: productpage-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;productpage.deployment.com\u0026#34; You can run the following command to create the Gateway:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/productpage-gateway.yaml Then emulate traffic flowing from outside of the cluster:\nwatch -x -n 0.1 curl -Is -H \u0026#39;Host: productpage.deployment.com\u0026#39; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; 2. Canary rollout configuration As specified in the targetService section of the following Experiment configuration, we have kubernetes service productpage directing traffic to deployments productpge-v1 and productpage-v2. The entry in hosts tells the controller that traffic will come through \u0026quot;productpage.deployment.com\u0026quot; configured in gateway(istio) paroductpage-gateway:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: productpage-v2-rollout spec: targetService: name: productpage baseline: productpage-v1 candidate: productpage-v2 port: 9080 hosts: - name: \u0026#34;productpage.deployment.com\u0026#34; gateway: paroductpage-gateway trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 3.0 sampleSize: 5 To create this Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_productpage-v1_to_productpage-v2.yaml 3. Deploy productpage-v2 and start the rollout To deploy the candidate version, productpage-v2, run the command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/productpage-v2.yaml Now check the state of the Experiment object. You should see that the rollout is in progress, and that a portion of the traffic is being sent to productpage-v2.\n$ kubectl get experiment productpage-v2-rollout -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-v2-rollout Progressing IterationUpdate: Iteration 1 Started productpage-v1 80 productpage-v2 20 Cleanup You can cleanup by deleting the namespace:\nkubectl delete ns bookinfo-iter8 "},{"uri":"https://iter8.tools/docs/archive/v1.0.0/getting-started/","title":"Getting started","tags":[],"description":"","content":"Tutorials  Getting started with canary testing  Learn how to perform a canary release\n   Automated canary releases with iter8 on Kubernetes and Istio  Perform a canary releases by shifting traffic to a canary version of a microservice.\n   Installation  Installation Iter8 on Kubernetes and Istio Learn how to install iter8 on Kubernetes and Istio Iter8 on Red Hat OpenShift Learn how to install iter8 on Red Hat OpenShift   Iter8 on Kubernetes and Istio  Learn how to install iter8 on Kubernetes and Istio\n   Iter8 on Red Hat OpenShift  Learn how to install iter8 on Red Hat OpenShift\n    Automated Canary Rollout Using Services  Perform a canary rollout when different versions have different service names\n   "},{"uri":"https://iter8.tools/docs/archive/v1.0.0/getting-started/installation/","title":"Installation","tags":[],"description":"","content":"Installation  Iter8 on Kubernetes and Istio  Learn how to install iter8 on Kubernetes and Istio\n   Iter8 on Red Hat OpenShift  Learn how to install iter8 on Red Hat OpenShift\n   "},{"uri":"https://iter8.tools/docs/archive/v1.0.0/getting-started/installation/red-hat/","title":"Iter8 on Red Hat OpenShift","tags":[],"description":"","content":"These instructions show you how to set up iter8 on Red Hat OpenShift.\nPrerequisites We recommend using the Red Hat OpenShift Service Mesh. This can be installed using the Red Hat OpenShift Service Mesh Operator. For details, see: https://docs.openshift.com/container-platform/4.3/service_mesh/service_mesh_install/installing-ossm.html.\nInstalling the Service Mesh involves installing the Elasticsearch, Jaeger, Kiali and Red Hat OpenShift Service Mesh Operators, creating and managing a ServiceMeshControlPlane resource to deploy the control plane, and creating a ServiceMeshMemberRoll resource to specify the namespaces associated with the Red Hat OpenShift Service Mesh.\nInstalling iter8 By default, iter8 uses the Prometheus service installed as part of the Red Hat OpenShift Service Mesh for the metrics used to assess the quality of different versions of a service. The Red Hat OpenShift Service Mesh configures the Prometheus service to require authentication. To configure iter8 to authenticate with Prometheus, some additional steps are needed.\nInstall the iter8 analytics service Download and untar the helm chart for the iter8-analytics service. The following options can be used to generate the needed yaml:\nREPO=iter8/iter8-analytics PROMETHEUS_SERVICE=\u0026#39;https://prometheus.istio-system:9090\u0026#39; PROMETHEUS_USERNAME=\u0026#39;internal\u0026#39; PROMETHEUS_PASSWORD=\u0026lt;FILL IN\u0026gt; helm template install/kubernetes/helm/iter8-analytics \\  --name iter8-analytics \\  --set image.repository=${REPO} \\  --set image.tag=v0.2.1 \\  --set iter8Config.authentication.type=basic \\  --set iter8Config.authentication.username=${PROMETHEUS_USERNAME} \\  --set iter8Config.authentication.password=${PROMETHEUS_PASSWORD} \\  --set iter8Config.authentication.insecure_skip_verify=true \\  --set iter8Config.metricsBackendURL=${PROMETHEUS_SERVICE} \\ | kubectl -n iter8 apply -f - The password, to be used can be found in the secret htpasswd in the namespace where Istio is installed. For example, the following might work to identify it:\nPROMETHEUS_PASSWORD=$(kubectl -n istio-system get secret htpasswd -o jsonpath=\u0026#39;{.data.rawPassword}\u0026#39; | base64 --decode) Install the iter8 controller The quick install instructions can be used to install the iter8 controller. The Service Mesh currently uses Istio telemetry version v1:\nkubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/iter8-controller.yaml Target Services The Red Hat OpenShift Service Mesh is restricted to the set of namespaces defined in the ServiceMeshMemberRoll resource. In particular, if you will be trying the tutorials, add the namespace bookinfo-iter8 to the ServiceMeshMemberRoll.\nIstio relies a sidecar injected into each pod to provide its capabilities. Istio provides several ways this sidecar can be injected. Red Hat recommends the use of the annotation sidecar.istio.io/inject: \u0026quot;true\u0026quot; in the deployment yaml. Examples can be found in the yaml for the tutorial: https://github.com/iter8-tools/iter8-controller/blob/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-tutorial.yaml\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/reference/experiment/","title":"Iter8&#39;s experiment CRD","tags":[],"description":"","content":"Coming soon!\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/getting-started/services/","title":"Automated Canary Rollout Using Services","tags":[],"description":"","content":"In iter8 the versions of a service being compared can be specified using deployment names or using service names. Other tutorials showed how to specify different versions using Kubernetes deployment names. In this tutorial, we learn how to do a canary rollout of an application when different versions are indicated by different Kubernetes service names.\nIn this tutorial, we again consider the user facing service productpage of the bookinfo application and we learn how to create an iter8 Experiment that specifies the baseline and candidate versions using Kubernetes services. The scenario we consider is here:\nIn this example, the application productpage.example.com can be routed, via an Istio Gateway and VirtualService, to the Kubernetes services. Iter8 can be used to automate the rollout including the creation of the Istio VirtualService.\nStep 1: Deploy the bookinfo Application Create a new namespace: $NAMESPACE. We use the name bookinfo-serivce.\nexport NAMESPACE=bookinfo-service kubectl create ns $NAMESPACE kubectl label ns $NAMESPACE istio-injection=enabled Deploy the bookinfo application to a new namespace. In particular, we create the service productpage-v1 to access the productpage application.\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-tutorial.yaml -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/productpage-v1.yaml Step 2: Configure Traffic to the Application Create an Istio gateway for the external host productpage.example.com:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/bookinfo-gateway.yaml At this point, the application is not actually accessible to users because no VirtualService has been defined. Rather than manually define it, this tutorial shows how iter8 can be used to create it for us. To do so, define an iter8 canary experiment from the current version of the application to itself. Clearly, this will succeed, and, as a side effect, a VirtualService will be created.\nIn the experiment, the targetService will look like:\ntargetService: kind: Service baseline: productpage-v1 candidate: productpage-v1 hosts: - name: productpage.example.com gateway: productpage-service It identifies the type of the baseline and candidate as services using kind: Service. The baseline and candidate names are the same. Further, it identifies the external host name and the Istio Gateway already configured.\nTo optimize the bootstrapping process, we can eliminate all of the successCriteria. This has the further benefit of eliminating the need for user traffic. We can also alter the trafficControl options to reduce the time and number of iterations required.\nYou can apply an optimized Experiment using:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/bootstrap-productpage.yaml You can verify that the Experiment has been created and finishes quickly:\nkubectl -n $NAMESPACE get experiment productpage-bootstrap NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-bootstrap Completed ExperimentSucceeded: Last Iteration Was Completed productpage-v1 0 productpage-v1 100 You can also verify that a virtual service has been created:\nkubectl -n $NAMESPACE get virtualservice NAME GATEWAYS HOSTS AGE productpage.example.com.iter8-experiment [productpage-service] [productpage.example.com] 20m This approach may seem unintuitive. However, we illustrate it here because this bootstrapping issue often arises when automating the use of canary rollouts.\nStep 3: Create an iter8 Canary Experiment We can now create a canary Experiment from version productpage-v1 to productpage-v2. The following command will create the Experiment:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/canary_productpage-v1_to_productpage-v2.yaml You can verify that the Experiment has been created:\nkubectl -n $NAMESPACE get experiment productpage-v2-rollout NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-v2-rollout Pause TargetsNotFound: Missing Candidate productpage-v1 100 productpage-v2 0 The experiment is paused since only the baseline version can be identified. When the candidate version is detected, the experiment will automatically begin execution.\nStep 4: Generate load As in earlier tutorials, emulate requests coming from users using curl:\nwatch -x -n 0.1 curl -Is -H \u0026#39;Host: productpage.example.com\u0026#39; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; Step 5: Deploy the candidate version productpage-v2 To start the rollout of the new version of the productpage application, deploy the new version:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/productpage-v2.yaml -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/productpage-v2.yaml You can verify the experiment has started:\nkubectl -n $NAMESPACE get experiment productpage-v2-rollout NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-v2-rollout Progressing IterationUpdate: Iteration 1 Started productpage-v1 80 productpage-v2 20 You can also verify that the VirtualService created by the bootstrap step has been reused:\nkubectl -n $NAMESPACE get virtualservice NAME GATEWAYS HOSTS AGE productpage.example.com.iter8-experiment [productpage-service] [productpage.example.com] 20m As the canary rollout progresses, you should see traffic shift from the baseline to the candidate version until all of the traffic is being sent to the new version.\nCleanup You can cleanup by deleting the namespace:\nkubectl delete ns $NAMESPACE "},{"uri":"https://iter8.tools/docs/archive/v1.0.0/reference/algorithms/","title":"Iter8&#39;s algorithms","tags":[],"description":"","content":"Coming soon!\n"},{"uri":"https://iter8.tools/docs/archive/v1.0.0/reference/","title":"Reference","tags":[],"description":"","content":"Reference  Iter8\u0026#39;s metrics  Coming soon!\n   Iter8\u0026#39;s experiment CRD  Coming soon!\n   Iter8\u0026#39;s algorithms  Coming soon!\n   "},{"uri":"https://iter8.tools/docs/archive/v1.0.0/releases/","title":"Older releases","tags":[],"description":"","content":" v1.0.0 v0.2.1 v0.2.0 v0.1.1 v0.1.0 v0.0.1  "},{"uri":"https://iter8.tools/docs/archive/v1.0.0/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://iter8.tools/docs/archive/v1.0.0/tags/","title":"Tags","tags":[],"description":"","content":""}]